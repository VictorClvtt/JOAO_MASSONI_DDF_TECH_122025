# Case T√©cnico Dadosfera - Data Engineer Jr
Case desenvolvido por **Jo√£o Victor Clivatti Massoni**

## Sum√°rio
- [Item 0 - Agilidade e Planejamento](#item-0---agilidade-e-planejamento)
- [Item 1 - Base de Dados](#item-1---base-de-dados)
- [Item 2 - Integra√ß√£o](#item-2---integra√ß√£o)
- [Item 3 - Explora√ß√£o](#item-3---explora√ß√£o)
- [Item 4 - Data Quality](#item-4---data-quality)
- [Item 5 - Processamento](#item-5---processamento)
- [Item 6 - Modelagem](#item-6---modelagem)
- [Item 7 - An√°lise](#item-7---an√°lise)
- [Item 8 - Pipelines](#item-8---pipelines)
- [Item 9 - Data Apps](#item-9---data-apps)
- [Item 10 - Apresenta√ß√£o](#item-10---apresenta√ß√£o)
- [Item Bonus - GenAI + Data Apps](#item-bonus---genai--data-apps)

## Item 0 - Agilidade e Planejamento
Abaixo est√° o diagrama de todo o case representando o meu entendimento e planejamento do case como todo:
![](./docs/diagrams/item_0/fluxograma_case.png)

## Item 1 - Base de Dados
Na escolha do conjunto de dados optei por criar um script Python que gera e salva localmente uma base de dados sint√©ticos sobre vendas, que seriam vendas da empresa de e-commerce a qual a defini√ß√£o do case se refere.

O script gera 125 mil linhas de dados que apesar de sint√©ticos seguem uma coer√™ncia entre si, estanto de acordo com os requisitos definidos pelo case.

O arquivo do script esta localizado em [./scripts/bronze/generate_data.py](./scripts/bronze/generate_data.py) e os arquivos de dados em CSV que ele grava est√£o localizados em [./data/raw/synthetic_data_full.csv](./data/raw/synthetic_data_full.csv)(sendo o conjunto de dados completo) e [./data/raw/synthetic_data_sample.csv](./data/raw/synthetic_data_sample.csv)(sendo uma amostra de mil linhas do conjunto completo).

## Item 2 - Integra√ß√£o
Na etapa de integra√ß√£o e ingest√£o da base de dados gerada na plataforma da Dadosfera, inicialmente criei um script([./scripts/ingest.py](./scripts/ingest.py)) que grava os dados que est√£o armazenados localmente em uma planilha do Google Sheets no meu Google Drive pessoal para poder conectar a Dadosfera √† essa planilha mais facilmente por estar na nuvem.

Antes mesmo de criar o script, comecei configurando a minha conta do Google para poder me comunicar com a API do Google, comecei ativando as APIs do Google Drive e do Google Sheets na se√ß√£o "APIs e servi√ßos" do Google Cloud Console:
![](./docs/prints/item_2/01_ativar_apis.png)

Ainda na se√ß√£o "APIs e servi√ßos", segui criando e posteriormente baixando credenciais pessoais para poder me comunicar com a API:
![](./docs/prints/item_2/02_opcao_criar_credencial_oauth.png)
![](./docs/prints/item_2/03_criar_credencial_oauth.png)
![](./docs/prints/item_2/04_baixar_credenciais.png)

Para finalmente poder me comunicar com as APIs, movi o arquivo baixado de credenciais para a raiz do diret√≥rio do projeto e o renomeei para [client_secret.json](./client_secret.json)

![](./docs/prints/item_2/05_renomear_e_mover_credenciais.png)

Com isso pude finalmente executar o script que move os dados para o Google Sheets, precisando apenas autorizar o script a manipular arquivos no meu Google Drive para que fosse criado o arquivo [token.json](./token.json) e n√£o fosse necess√°ria essa confirma√ß√£o em execu√ß√µes futuras. Para executar o script deve-se executa-lo como m√≥dulo: ```python -m scripts.bronze.generate_data``` dentro do diret√≥rio raiz do projeto (lembrando que o venv deve ter sido criado e ativado) com os seguintes parametros
```python
if __name__ == "__main__":
    ingest_data_to_sheets(
        csv_file="./data/raw/synthetic_data_full.csv",
        secret_path="client_secret.json",
        env_path=".env",
        spreadsheet_name="raw_orders"
    )
```
![](./docs/prints/item_2/06_autorizar_execucao.png)
![](./docs/prints/item_2/07_execucao_autorizada.png)

Ap√≥s o script ser executado com sucesso, pude verificar no meu Google Drive que a planilha com os dados havia sido gravada com sucesso.
![](./docs/prints/item_2/08_dados_salvos_no_google_drive.png)

At√© esse ponto minha id√©ia era poder criar scripts que executassem toda a etapa de integra√ß√£o, desde os dados locais at√© os dados ingeridos na plataforma, por√©m infelizmente enfrentei muitas dificuldades utilizando a API da Dadosfera, senti que faltou uma melhor descri√ß√£o e talvez exemplos de como consumir os endpoints da API e que tipo de valores deveriam ser colocados em cada campo(o m√°ximo que consegui foi criar uma fun√ß√£o para logar na API). Por isso optei por criar a conex√£o com o Google Sheets e executar a pipeline de integra√ß√£o manualmente pelo site.

Para terminar a integra√ß√£o dos dados, primeiro criei uma conex√£o com o Google Sheets manualmente na se√ß√£o de conex√µes da plataforma da Dadosfera:

Comecei clicando em "Nova fonte"
![](./docs/prints/item_2/09_opcao_nova_fonte.png)

Em seguida selecionei o Google Sheets como fonte
![](./docs/prints/item_2/10_selecionar_google_sheets.png)

E preenchi as informa√ß√µes da conex√£o
![](./docs/prints/item_2/11_informacoes_gerais_da_fonte.png)
![](./docs/prints/item_2/12_autorizar_google_na_fonte.png)

Depois de preencher as informa√ß√µes da conex√£o consegui cria-la com sucesso
![](./docs/prints/item_2/13_conexao_criada.png)

Com a conex√£o criada parti para a cria√ß√£o da pipeline que foi a √∫ltima etapa da integra√ß√£o:

Comecei clicando em "Nova pipeline" e depois em "Criar pipeline" no modal que o primeiro bot√£o abriu
![](./docs/prints/item_2/14_opcao_criar_pipeline.png)

E preenchi as informa√ß√µes da pipeline
![](./docs/prints/item_2/15_informacoes_gerais_da_pipeline.png)
![](./docs/prints/item_2/16_inserir_link_dos_dados.png)
![](./docs/prints/item_2/17_preencher_abas.png)
![](./docs/prints/item_2/18_definir_agendamento.png)

Depois de preencher as informa√ß√µes da pipeline, fazer outras tentativas trocando algumas configura√ß√µes e esperar um pouco finalmente a pipeline foi executada e os dados foram integrados com sucesso
![](./docs/prints/item_2/19_pipeline_executada.png)

## Item 3 - Explora√ß√£o
Pelo que pude perceber, os dados foram automaticamente catalogados na plataforma da Dadosfera ap√≥s a integra√ß√£o, a unica coisa que fiz foi alterar o nome da tabela no cat√°logo para "orders_landing_zone".

Com os dados j√° catalogados, pude explora-los atrav√©s da plataforma da Dadosfera que me trouxe algumas estat√≠sticas sobre cada coluna do conjunto e uma visualiza√ß√£o de uma amostra dos dados em formato de tabela:
![](./docs/prints/item_3/01_estatisticas_basicas.png)
![](./docs/prints/item_3/02_visualizacao_em_tabela.png)

Explorando os dados, pude ver os defeitos que eu mesmo inseri propositalmente neles j√° que eu fiz o script que os gera. Entre os defeitos inseridos est√£o valores nulos, valores fora de um alcance de valores l√≥gico, linhas duplicadas, endere√ßos de e-mail fora do padr√£o e nomes com espa√ßos em branco antes e depois do nome.

Tendo conhecimento sobre os defeitos da base de dados eu estava pronto para explora-los melhor com outras ferramentas.

## Item 4 - Data Quality
Para a etapa de verifica√ß√£o de qualidade de dados desenvolvi o script [data_quality_report.py](./scripts/silver/data_quality_report.py) que l√™ o conjunto de dados gerados pelo script [generate_data.py](./scripts/bronze/generate_data.py) em CSV com Pandas e faz valida√ß√µes de qualidade de dados com Great Expectations.

Para executar o script deve-se executa-lo como m√≥dulo: ```python -m scripts.silver.data_quality_report``` dentro do diret√≥rio raiz do projeto (lembrando que o venv deve ter sido criado e ativado)

As valida√ß√µes foram definidas com base em um Common Data Model (CDM) orientado a entidades de neg√≥cio, cobrindo regras de integridade, consist√™ncia e sem√¢ntica dos dados.

Foram realizadas as seguintes verifica√ß√µes sobre o conjunto de dados:

**Customer:**
- Verifica√ß√£o de valores nulos nos campos customer_id, customer_name e customer_email.
- Valida√ß√£o de higiene de strings, garantindo que o campo customer_name n√£o contenha espa√ßos em branco no in√≠cio ou no final.
- Valida√ß√£o do formato de e-mail, assegurando que os valores em customer_email sigam o padr√£o esperado.
- Valida√ß√£o de dom√≠nio, garantindo que o estado do cliente (customer_state) perten√ßa ao conjunto de estados v√°lidos.

**Order:**
- Verifica√ß√£o de unicidade do identificador do pedido (order_id), evitando registros duplicados.
- Verifica√ß√£o de valores nulos nos campos order_id e order_date.
- Valida√ß√£o de dom√≠nio do status do pedido (status), restringindo os valores a um conjunto previamente definido.
- Valida√ß√£o de integridade referencial, garantindo a presen√ßa de customer_id associado ao pedido.

**Order Item:**
- Verifica√ß√£o de valores nulos nos campos product_id, order_id, quantity e unit_price.
- Valida√ß√£o de intervalos l√≥gicos, garantindo que:
- A quantidade (quantity) seja maior ou igual a 1.
- O pre√ßo unit√°rio (unit_price) seja maior que zero, com toler√¢ncia controlada.

**Payment:**
- Valida√ß√£o de dom√≠nio do m√©todo de pagamento (payment_method), restringindo os valores aos meios aceitos.
- Verifica√ß√£o de integridade referencial, garantindo a associa√ß√£o do pagamento a um pedido (order_id).

**Shipping:**
- Valida√ß√£o de regras de neg√≥cio, assegurando que pedidos cancelados n√£o possuam data de envio (shipping_date).
- Verifica√ß√£o de consist√™ncia temporal, garantindo que a data de envio seja posterior √† data do pedido para pedidos n√£o cancelados.

**Review:**
- Valida√ß√£o de regras de neg√≥cio, garantindo que pedidos cancelados n√£o possuam avalia√ß√µes (review).

Ao final da execu√ß√£o, o script gera um relat√≥rio de valida√ß√£o no console, informando o status geral da qualidade dos dados, o total de expectativas avaliadas e o detalhamento das valida√ß√µes que apresentaram falhas:

![](./docs/prints/item_4/01_relatorio_de_qualidade_de_dados.png)

O pipeline de Data Quality foi executado com sucesso e identificou cinco viola√ß√µes relevantes nos dados de entrada:

- Nomes de clientes com espa√ßos em branco antes e/ou depois do nome
- Endere√ßos de e-mail fora do padr√£o
- Id de compra duplicados
- Valores fora de uma faixa de valores l√≥gica no campo 'quantity'
- Valores nulos de valor unit√°rio
- Valores de data de compra antes de valores de data de entrega e vice versa

Para tratar esses defeitos nos dados optei por utilizar PySpark(Spark) por ser uma ferramenta muito adequada para cen√°rios de produ√ß√£o e de grandes volumes de dados, e tamb√©m por ser uma ferramenta que j√° uso h√° alguns anos(apesar de tamb√©m trabalhar com Pandas, R, etc).

Desenvolvi um script com PysPark que l√™ os dados crus, corrige da forma mais adequada poss√≠vel(procurando recuperar dados) cada um dos defeitos que encontrei com o Great Expectations e grava o dataset limpo localmente em [data/clean/clean_orders/](./data/clean/clean_orders/). O script pode ser executado como m√≥dulo com o comando ```python -m scripts.silver.clean``` dentro do diret√≥rio raiz do projeto (lembrando que o venv deve ter sido criado e ativado e √© necess√°rio ter um JDK insatalado para executar Spark, de prefer√™ncia o 17).

Agora se mudarmos os parametros da funcao de data quality para que ela verifique a qualidade de dados sobre os dados limpos pelo script PySpark ser;a poss√≠vel observar que o conjunto de dados limpos passa em todas as condi√ß√µes de qualidade de dados definidas no script as quais os dados crus n√£o passavam:
```python
if __name__ == "__main__":
    # data_quality_report(dataset="raw", dataset_path="./data/raw/synthetic_data_full.csv")
    data_quality_report(dataset="clean", dataset_path="./data/clean/clean_orders/*.csv")
```
![](./docs/prints/item_4/03_relatorio_de_qualidade_de_dados_limpos.png)

Ap√≥s isso fiz o mesmo procedimento do [Item 2](#item-2---integra√ß√£o), primeiro executei o script que carrega os dados([scripts/ingest.py](./scripts/ingest.py)) no Google Sheets com outros par√¢metros como mostrado abaixo e depois fiz as etapas manuais na plataforma para integrar e catalogar tamb√©m os dados limpos na plataforma da Dadosfera.
```python
if __name__ == "__main__":
    ingest_data_to_sheets(
        csv_folder="./data/clean/clean_orders",
        secret_path="client_secret.json",
        env_path=".env",
        spreadsheet_name="clean_orders"
    )
```

## Item 5 - Processamento
O dataset que escolhi para esse item √© o dataset que baixei para o diret√≥rio [./data/raw/](./data/raw/) e que se encontra para download [nesse link](https://raw.githubusercontent.com/octaprice/ecommerce-product-dataset/main/data/mercadolivre_com_br/reviews_mercadolivre_com_br_1.json), o dataset se trata de um pouco mais de 100 mil registros em JSON de avalia√ß√µes de produtos do Mercado Livre, cada registro contendo a data da avalia√ß√£o, uma nota num√©rica, uma avalia√ß√£o textual e o link original produto avaliado, sendo poss√≠vel extrair features dos campos de avalia√ß√£o textual e do link.

Para executar o c√≥digo de extra√ß√£o de features optei por utilizar o Google Colab como indicado pela documenta√ß√£o do case por ser um ambiente mais preparado para esse tipo de c√≥digo.

Por estar trabalhando com IA, utilizei modelos do HuggingFace, o que me fez precisar criar uma conta no HuggingFace, criar um token e registrar esse token com o nome 'HF_TOKEN' no Colab para que eu pudesse utilizar os modelos.

Criei um diret√≥rio chamado 'data' e dentro dele outro diret√≥rio chamado 'raw' e copiei o [arquivo JSON](./data/raw/reviews_mercadolivre_com_br_1.json) para dentro desse diret√≥rio manualmente.

Em seguida, desenvolvi um [script](./scripts/bronze/extract_features.py) que utiliza intelig√™ncia artificial para extrair informa√ß√µes adicionais de forma program√°tica a partir dos campos content e product_url do JSON original. Entre essas informa√ß√µes, est√£o: uma lista de t√≥picos relacionados ao produto, um campo que indica se o usu√°rio recomenda ou n√£o o produto, e uma an√°lise de sentimento que classifica a avalia√ß√£o como muito negativa, negativa, neutra, positiva ou muito positiva.""Em seguida, desenvolvi um script que utiliza duas intelig√™ncias artificiais para enriquecer os dados extra√≠dos do JSON original (content e product_url).

- A primeira IA, ```tabularisai/multilingual-sentiment-analysis```, foi utilizada para an√°lise de sentimento, classificando cada avalia√ß√£o como muito negativa, negativa, neutra, positiva ou muito positiva.
Al√©m disso, o script gera um campo que indica se o usu√°rio recomenda ou n√£o o produto, com base no sentimento detectado.

- A segunda IA, ```MoritzLaurer/mDeBERTa-v3-base-mnli-xnli```, foi aplicada em uma tarefa de zero-shot classification para identificar t√≥picos relevantes mencionados pelo usu√°rio no review e no nome do produto.

Por fim o script grava esse novo JSON com as features dentro do mesmo diret√≥rio do original.

Neste reposit√≥rio √© poss√≠vel acessar apenas uma [amostra dos dados](./data/raw/reviews_with_features_sample.json) com features pois infelizmente o processamento que o Colab me disponibilizou n√£o me possibilitou processar os 100 mil registros.

## Item 6 - Modelagem
Para definir um modelo de dados sobre o conjunto de dados de vendas optei por come√ßar desenvolvendo um script PySpark que modela os dados posteriormente limpos por outro script PySpark pois estou mais acostumado a olhar o schema dos dados por meio do c√≥digo e a partir disso modelar as tabelas.

### Modelagem definida:
Durante o desenvolvimento do [script de modelagem](./scripts/gold/data_modeling.py) eu desenvolvi o seguinte modelo:
```
fact_order schema:
root
 |-- order_id: integer (nullable = true)
 |-- order_date_sk: string (nullable = true)
 |-- shipping_date_sk: string (nullable = true)
 |-- customer_sk: string (nullable = true)
 |-- product_sk: string (nullable = true)
 |-- payment_method_sk: string (nullable = true)
 |-- order_status_sk: string (nullable = true)
 |-- quantity: integer (nullable = true)
 |-- unit_price: double (nullable = true)
 |-- total_amount: double (nullable = true)

dim_date schema:
root
 |-- full_date: date (nullable = true)
 |-- day: integer (nullable = true)
 |-- month: integer (nullable = true)
 |-- month_name: string (nullable = true)
 |-- quarter: integer (nullable = true)
 |-- year: integer (nullable = true)
 |-- day_of_week: string (nullable = true)
 |-- is_weekend: boolean (nullable = false)
 |-- date_sk: string (nullable = true)

dim_order_status schema:
root
 |-- status: string (nullable = true)
 |-- order_status_sk: string (nullable = true)

dim_payment_method schema:
root
 |-- payment_method: string (nullable = true)
 |-- payment_method_sk: string (nullable = true)

dim_product schema:
root
 |-- product_id: integer (nullable = true)
 |-- product_name: string (nullable = true)
 |-- category: string (nullable = true)
 |-- supplier: string (nullable = true)
 |-- product_sk: string (nullable = true)

dim_customer schema:
root
 |-- customer_id: integer (nullable = true)
 |-- customer_name: string (nullable = true)
 |-- customer_email: string (nullable = true)
 |-- customer_sk: string (nullable = true)
 |-- location_sk: string (nullable = true)

dim_location schema:
root
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- location_sk: string (nullable = true)
```
Estes s√£o os schemas de cada tabela definida para o modelo dimensional de dados que visa identificar cada entidade de dados e suas vari√°veis e tornar consultas anal√≠ticas mais eficientes.

A modelagem proposta segue os princ√≠pios de Dimensional Modeling (Kimball), adotando um Snowflake Schema, com uma tabela fato central (fact_order) e dimens√µes descritivas ao redor.
Essa abordagem foi escolhida por favorecer simplicidade, performance anal√≠tica e facilidade de consumo por ferramentas de BI, sendo adequada ao cen√°rio de pedidos e vendas do cliente.

### Vis√µes finais dos dados:
- üìä Vis√£o 1 ‚Äì Vendas por per√≠odo e produto
```SQL
SELECT
    d.year,
    d.month,
    p.category,
    p.product_name,
    SUM(f.quantity) AS total_quantity,
    SUM(f.total_amount) AS total_revenue
FROM fact_order f
JOIN dim_date d ON f.order_date_sk = d.date_sk
JOIN dim_product p ON f.product_sk = p.product_sk
GROUP BY d.year, d.month, p.category, p.product_name;
```
Vendas por per√≠odo e produto permite analisar o desempenho comercial ao longo do tempo, detalhando vendas e receita por produto e categoria, apoiando decis√µes relacionadas a planejamento, estoque e estrat√©gia de vendas.

- üë§ Vis√£o 2 ‚Äì Comportamento do cliente
```SQL
SELECT
    c.customer_name,
    l.city,
    l.state,
    COUNT(DISTINCT f.order_id) AS total_orders,
    SUM(f.total_amount) AS total_spent
FROM fact_order f
JOIN dim_customer c ON f.customer_sk = c.customer_sk
JOIN dim_location l ON c.location_sk = l.location_sk
GROUP BY c.customer_name, l.city, l.state;
```
Comportamento do cliente possibilita analisar padr√µes de consumo dos clientes, considerando volume de pedidos, valor gasto e localiza√ß√£o geogr√°fica, sendo √∫til para segmenta√ß√£o, marketing e identifica√ß√£o de clientes estrat√©gicos.

### Diagrama da Modelagem Diomensional:
Baseado no modelo que desenvolvi no script, criei um diagrama relacional do modelo de dados:
![](./docs/diagrams/item_6/diagrama_dw.png)

## Item 7 - An√°lise
## Item 8 - Pipelines
## Item 9 - Data Apps
## Item 10 - Apresenta√ß√£o
## Item Bonus - GenAI + Data Apps